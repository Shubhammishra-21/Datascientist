{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\shubham\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (20.3.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (19.0.0)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (4.4.1)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (5.1.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from scrapy) (2.7)\n",
      "Requirement already satisfied: six in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.12.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (19.2.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (19.0.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (41.4.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.12.3)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\shubham\\anaconda3\\lib\\site-packages (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\shubham\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\shubham\\anaconda3\\lib\\site-packages (from selenium) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.com/s?k=sarees+for+women&crid=7VIKQHPTG4B4&sprefix=sarees%2Caps%2C399&ref=nb_sb_ss_i_1_6'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "aas = soup.find_all(\"a\", class_='entry-featured-image-url')\n",
    "driver = webdriver.Chrome('C:/Users/Shubham/AppData/Local/Temp/Rar$EXa0.328/Chromedriver')\n",
    "driver.get('https://www.amazon.com/s?k=sarees+for+women&crid=7VIKQHPTG4B4&sprefix=sarees%2Caps%2C399&ref=nb_sb_ss_i_1_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_info = []\n",
    "\n",
    "for a in aas:\n",
    "    image_tag = a.findsarees(\"img\")\n",
    "    image_info.append((image_tag[0][\"src\"], image_tag[0][\"alt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image):\n",
    "    response = requests.get(image[0], stream=True)\n",
    "    realname = ''.join(e for e in image[1] if e.isalnum())\n",
    "    \n",
    "    file = open(\"C://images//bs//{}.jpg\".format(realname), 'wb')\n",
    "    \n",
    "    response.raw.decode_content = True\n",
    "    shutil.copyfileobj(response.raw, file)\n",
    "    del response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(image_info)):\n",
    "    download_image(image_info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}\n",
    "IMAGES_STORE = 'C:/images/scrapy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ImageItem(scrapy.Item):\n",
    "    images = scrapy.Field()\n",
    "    image_urls = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ImgSpider(scrapy.spiders.Spider):\n",
    "    name = \"img_spider\"\n",
    "    start_urls = ['https://www.amazon.com/s?k=sarees+for+women&crid=7VIKQHPTG4B4&sprefix=sarees%2Caps%2C399&ref=nb_sb_ss_i_1_6']\n",
    "\n",
    "    def parse(self, response):\n",
    "        image = ImageItem()\n",
    "        img_urls = []\n",
    "\n",
    "        for img in response.css(\".entry-featured-image-url img::attr(src)\").extract():\n",
    "            img_urls.append(img)\n",
    "\n",
    "        image[\"image_urls\"] = img_urls\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-84-85ccfcaa0c12>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-84-85ccfcaa0c12>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    scrapy crawler img_spyder\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scrapy crawler img_spyder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-85-dd40f0e7975c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-85-dd40f0e7975c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    scrapy runspider ImgSpyder.py\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scrapy runspider ImgSpyder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
